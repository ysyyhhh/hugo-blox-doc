
## 经典问题

拜占庭将军问题
描述： 一组将军围攻一座城市，他们需要达成一致的决策，但是其中一些将军可能是叛徒，他们会向其他将军发送错误的信息。这个问题的目标是找到一种算法，使得所有忠诚的将军能够达成一致的决策，即使有叛徒存在。
经典设置：
1. 每个将军都是一个节点，他们之间通过消息进行通信。
2. 每个将军都有一个二值的决策，即攻击或者撤退。
3. 每个将军都知道自己是不是叛徒，但是不知道其他将军是不是叛徒。
4. 叛徒将军可以发送错误的消息。
5. 总共有m个将军，其中n个是忠诚的，m-n个是叛徒。
6. 问题的目标是找到一种算法，使得所有忠诚的将军能够达成一致的决策，即使有叛徒存在。


## CAP & Base理论

### CAP 理论

CAP理论是分布式系统设计中的一个重要理论，它指出在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）这三个要素不可兼得，最多只能同时满足其中的两个。

- 一致性（Consistency）：在分布式系统中的所有节点上的数据是一致的，即一个节点上的数据发生了变化，那么在其他节点上也应该立即发生变化。
- 可用性（Availability）：分布式系统中的所有节点都能够正常响应客户端的请求。
- 分区容错性（Partition tolerance）：分布式系统中的节点之间可能会因为网络分区而无法通信，分区容错性指的是系统能够继续工作，即使节点之间无法通信。

网络分区: 分布式系统中，多个节点之间的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫 网络分区。

CAP一般要满足两个，而P是必须的，所以CAP理论一般是AP或者CP。

- CP：HBase、MongoDB等分布式数据库是CP系统，它要求数据一致性和分区容错性，但是无法保证所有节点的可用性。
- AP：Cassandra、DynamoDB等分布式数据库是AP系统，它要求可用性和分区容错性，但是无法保证数据一致性。

一致性与可用性的矛盾
- 一致性要求所有节点看到的数据都是相同的,这意味着在更新数据时需要等待所有节点的确认。
- 而可用性要求系统在出现网络分区时仍然能够对外提供服务,这就意味着不能等待所有节点的确认。
在网络分区的情况下,这两个要求是矛盾的。

注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。

常见的可以作为注册中心的组件有：ZooKeeper、Eureka、Nacos

ZooKeeper保证的是CP，Eureka保证的是AP，Nacos既可以保证CP也可以保证AP。


### Base 理论

Base理论是对CAP理论的一个补充，它是指在分布式系统中，基本的理论是基于CAP理论，但是在实际的分布式系统中，我们往往无法做到强一致性，而是通过牺牲强一致性来换取可用性或者分区容错性。

BASE 是 Basically Available（基本可用）、Soft-state（软状态） 和 Eventually Consistent（最终一致性）

什么叫允许损失部分可用性呢？
- 响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
- 系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。

软状态指
允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。


分布式一致性的 3 种级别：
- 强一致性：系统写入了什么，读出来的就是什么。
- 弱一致性：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。
- 最终一致性：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。


实现最终一致性的方法：

- 读时修复 : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据。
- 写时修复 : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。
- 异步修复 : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。



## 分布式算法

Paxos算法

Raft算法
- https://javaguide.cn/distributed-system/protocol/raft-algorithm.html#_2-3-%E6%97%A5%E5%BF%97



## 分布式ID

一个最基本的分布式 ID 需要满足下面这些要求：

- 全局唯一：ID 的全局唯一性肯定是首先要满足的！
- 高性能：分布式 ID 的生成速度要快，对本地资源消耗要小。
- 高可用：生成分布式 ID 的服务要保证可用性无限接近于 100%。
- 方便易用：拿来即用，使用方便，快速接入！

### 解决方案

#### 数据库主键自增

基于数据库的号段模式来生成分布式 ID。

NoSQL 方案使用 Redis 多一些。我们通过 Redis 的 incr 命令即可实现对 id 原子顺序递增

#### UUID

UUID 是 Universally Unique Identifier（通用唯一标识符） 的缩写。UUID 包含 32 个 16 进制数字（8-4-4-4-12）。JDK 就提供了现成的生成 UUID 
```java
//输出示例：cb4a9ede-fa5e-4585-b9bb-d60bce986eaa
UUID.randomUUID()
```

#### 开源框架 雪花算法/号段模式

- 雪花算法

雪花算法是 Twitter 开源的分布式 ID 生成算法，其核心思想是：使用一个 64 位的 long 型的数字作为全局唯一 ID。在雪花算法中，64 位 long 型的数字中，其中 1 位是不用的，然后用其中的 41 位作为毫秒数，用 10 位作为工作机器 id，12 位作为序列号。

- 号段模式

号段模式是数据库号段模式的一种变种，号段模式的核心思想是：每次从数据库中取一段 id，然后在内存中递增生成 id，当这一段 id 用完了之后，再去数据库中取一段 id。



## 分布式锁

如何才能实现共享资源的互斥访问呢？ 锁是一个比较通用的解决方案，更准确点来说是悲观锁。

一个最基本的分布式锁需要满足：

互斥：任意一个时刻，锁只能被一个线程持有。
高可用：锁服务是高可用的，当一个锁服务出现问题，能够自动切换到另外一个锁服务。并且，即使客户端的释放锁的代码逻辑出现问题，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。这一般是通过超时机制实现的。
可重入：一个节点获取了锁之后，还可以再次获取锁。
除了上面这三个基本条件之外，一个好的分布式锁还需要满足下面这些条件：

高性能：获取和释放锁的操作应该快速完成，并且不应该对整个系统的性能造成过大影响。
非阻塞：如果获取不到锁，不能无限期等待，避免对系统正常运行造成影响。

### 实现方式
常见分布式锁实现方案如下：

基于关系型数据库比如 MySQL 实现分布式锁。
基于分布式协调服务 ZooKeeper 实现分布式锁。
基于分布式键值存储系统比如 Redis 、Etcd 实现分布式锁。
关系型数据库的方式一般是通过唯一索引或者排他锁实现。不过，一般不会使用这种方式，问题太多比如性能太差、不具备锁失效机制。

基于 ZooKeeper 或者 Redis 实现分布式锁这两种实现方式要用的更多一些，我专门写了一篇文章来详细介绍这两种方案：分布式锁常见实现方案总结。


## 分布式事务

两阶段提交是一种常用的分布式事务处理协议,它分为以下两个阶段:

1. 准备阶段(Phase 1):
   - 协调者(Coordinator)询问参与者(Participant)是否准备好提交事务。
   - 参与者检查自己是否准备好提交,并反馈给协调者。

2. 提交阶段(Phase 2):
   - 如果所有参与者都准备好了,协调者会发出提交命令。
   - 参与者执行提交操作,并向协调者发送提交完成通知。
   - 协调者收到所有参与者的提交完成通知后,整个事务提交成功。

在你描述的场景中,协调者在收到参与者提交完成通知后挂掉了,这种情况下参与者可以采取以下措施:

1. 重试:参与者可以尝试重复发送提交完成通知,直到协调者恢复。

2. 超时重试:如果长时间没有收到协调者的响应,参与者可以假设协调者已经挂掉,启动超时恢复机制。

3. 手动介入:如果前两种方法无法解决,可能需要人工介入,检查系统状态并决定如何进行事务恢复。

4. 使用日志:参与者可以记录事务的执行状态和结果,以便在协调者恢复后重现事务处理过程。

总之,两阶段提交协议提供了一种可靠的分布式事务处理机制,但在某些极端情况下,如协调者故障,参与者需要采取一定的措施来保证事务的一致性和完整性。

2PC 存在的问题：
同步阻塞 ：事务参与者会在正式提交事务之前会一直占用相关的资源。比如用户小明转账给小红，那其他事务也要操作用户小明或小红的话，就会阻塞。
数据不一致 ：由于网络问题或者TM宕机都有可能会造成数据不一致的情况。比如在第2阶段（提交阶段），部分网络出现问题导致部分参与者收不到 Commit/Rollback 消息的话，就会导致数据不一致。
单点问题 ： TM在其中也是一个很重要的角色，如果TM在准备(Prepare)阶段完成之后挂掉的话，事务参与者就会一直卡在提交(Commit)阶段。


### 三阶段提交

准备阶段
- 协调者向所有参与者发送事务内容，询问是否可以执行事务。

CanCommit 阶段
- 参与者收到协调者的 CanCommit 消息后，会执行事务，然后向协调者发送 Yes 消息。
- 如果参与者执行事务失败，会向协调者发送 No 消息。

预提交阶段
- 协调者收到所有参与者的 Yes 消息后，会向所有参与者发送预提交消息。
- 参与者收到预提交消息后，会执行事务，并向协调者发送 Ack 消息。

提交阶段
- 协调者收到所有参与者的 Ack 消息后，会向所有参与者发送 Commit 消息。
- 参与者收到 Commit 消息后，会提交事务。



分布式事务的中间件Seata

## TODO: 分布式缓存

### 一致性哈希算法

### Redis使用的哈希槽


